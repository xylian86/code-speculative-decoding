{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e94e4e-8ca0-462a-9f41-95a2f41a944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers # requires transformers==4.35.2\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1668d6c1-d608-4f91-8741-b33693e83d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e27a50-63e6-45c1-88c2-e43d2fa94bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/vijay/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4814b9bc5b6408993328c4135533a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16) # , use_flash_attention=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640f2d62-7976-4d26-beb1-08aa01043736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "\n",
    "from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n",
    "from transformers.models.auto import (\n",
    "    MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    "    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n",
    "    MODEL_FOR_VISION_2_SEQ_MAPPING,\n",
    ")\n",
    "from transformers.utils import ExplicitEnum, ModelOutput, is_accelerate_available, logging\n",
    "from transformers.generation.beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from transformers.generation.logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    EncoderRepetitionPenaltyLogitsProcessor,\n",
    "    EpsilonLogitsWarper,\n",
    "    EtaLogitsWarper,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    ForceTokensLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    MinNewTokensLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    SequenceBiasLogitsProcessor,\n",
    "    SuppressTokensAtBeginLogitsProcessor,\n",
    "    SuppressTokensLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    "    UnbatchedClassifierFreeGuidanceLogitsProcessor,\n",
    ")\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "\n",
    "from transformers.generation.utils import _crop_past_key_values\n",
    "import difflib\n",
    "\n",
    "@dataclass\n",
    "class GreedySearchDecoderOnlyOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of decoder-only generation models using greedy search.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
    "            if all batches finished early due to the `eos_token_id`.\n",
    "        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
    "            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
    "            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
    "        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
    "        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
    "            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    sequences: torch.LongTensor = None\n",
    "    scores: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5466bcc-865b-4333-9cae-4d3fa4afd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
    "    input_length = input_ids.size(1)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        # Extract the last n tokens as our search ngram\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "\n",
    "        # Create sliding windows of size ngram_size\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "\n",
    "        # Convert ngram to a tensor for comparison\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        # Find where the windows match the ngram\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "\n",
    "        # Get the indices of matches\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Iterate through match indices to find a valid continuation\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            # Ensure we don't go beyond the length of input_ids and avoid self-match\n",
    "            if end_idx <= input_length and start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:end_idx]\n",
    "\n",
    "    # If no match is found, return an empty tensor\n",
    "    return torch.tensor([], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "COLORS = [\"\\x1b[31m\", \"\\x1b[32m\", \"\\x1b[34m\", \"\\x1b[35m\"]  # Red, Green, Blue, Magenta\n",
    "UNDERLINE = \"\\x1b[4m\"\n",
    "RESET = \"\\x1b[0m\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_search_pld(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        draft_matching_window_size = 3,\n",
    "        draft_num_candidate_tokens = 10,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "\n",
    "        while True:\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            candidate_pred_tokens = find_candidate_pred_tokens(input_ids, draft_matching_window_size, draft_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "            \n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "            \n",
    "            if input_ids[0, -1] == 185 and input_ids[0, -2] == 185 and input_ids[0, -3] == 185: # hacky stopping criteria for new-line ending models\n",
    "                break\n",
    "            \n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                # attentions=decoder_attentions,\n",
    "                # hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfd3b3c4-7eb8-47f2-9bec-87d0a12bf98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32013, 1202]\n",
      "[32013, 185]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"...\"))\n",
    "print(tokenizer.encode(\"\"\"\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "575a21f7-2aed-47ab-9f27-7c52a3c506fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"\"\"import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the average\n",
    "average_throughput = np.mean(tokens_per_sec_arr)\n",
    "print(f\"Average Throughput: {average_throughput} tokens/sec\")\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram of Throughput Values')\n",
    "plt.xlabel('Tokens per Second')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.text(average_throughput*0.9, max(plt.ylim())*0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "question = \"Can you please change x axis to start from 0\"\n",
    "prompt = \"[INST] Code:```python\\n{code_text}``` \\n\\n Question: {question} \\n\\n Modified code:[/INST]\".format(code_text=code_text, question=question)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "code_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Move all tensor values in the inputs to GPU\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(device)\n",
    "    code_inputs[key] = code_inputs[key].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7068b630-c984-4665-b69a-921a83f7ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_search_pld_custom(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        code_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        draft_matching_window_size = 3,\n",
    "        draft_num_candidate_tokens = 10,\n",
    "        print_output=True,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "\n",
    "        global tokenizer\n",
    "\n",
    "        # init values\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "        # # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "\n",
    "        max_len = stopping_criteria[0].max_length\n",
    "\n",
    "        i = 0\n",
    "        current_color_index = 0\n",
    "        original_input_length = input_ids.shape[-1]\n",
    "\n",
    "        while True:\n",
    "            i += 1\n",
    "            cur_len = input_ids.shape[-1]\n",
    "\n",
    "            candidate_pred_tokens = find_candidate_pred_tokens_diff(input_ids, code_ids, original_input_length, draft_matching_window_size, draft_num_candidate_tokens)\n",
    "\n",
    "            if len(candidate_pred_tokens) == 0:\n",
    "                candidate_pred_tokens = torch.tensor([100], device=input_ids.device).unsqueeze(0)\n",
    "            else:\n",
    "                candidate_pred_tokens = candidate_pred_tokens.unsqueeze(0)\n",
    "\n",
    "            candidate_input_ids = torch.cat((input_ids, candidate_pred_tokens), dim=1)\n",
    "            \n",
    "            candidate_length = candidate_input_ids.shape[1] - input_ids.shape[1]\n",
    "\n",
    "            candidate_kwargs = copy.copy(model_kwargs)\n",
    "            candidate_kwargs = self._extend_attention_mask(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "            candidate_kwargs = self._extend_token_type_ids(candidate_kwargs, candidate_input_ids.shape[1])\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n",
    "            \n",
    "            # prepare model inputs\n",
    "            # model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "\n",
    "            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n",
    "            selected_tokens = new_logits.argmax(dim=-1)\n",
    "            candidate_new_tokens = candidate_input_ids[:, -candidate_length:]\n",
    "            n_matches = ((~(candidate_new_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1).sum()\n",
    "\n",
    "            \n",
    "            # if last_assistant_token_is_eos and n_matches == candidate_length: # todo: do this earlier somehow\n",
    "            #     n_matches -= 1\n",
    "            \n",
    "            n_matches = min(n_matches, max_len - cur_len - 1)\n",
    "\n",
    "            # print(n_matches)\n",
    "            # i+= n_matches.item()\n",
    "\n",
    "            if print_output:\n",
    "                current_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "            first_newline = -1\n",
    "            last_newline = 0\n",
    "            for i in range(n_matches + 1):\n",
    "                if valid_tokens[0, i] == 185:\n",
    "                    if first_newline < 0:\n",
    "                        first_newline = i\n",
    "                    last_newline = i\n",
    "\n",
    "            new_cache_size = -1\n",
    "            \n",
    "            if valid_tokens.shape[1] > 20: # just arbitrarily crop and use ellipses\n",
    "                # print(first_newline, last_newline)\n",
    "                st = valid_tokens.shape[1]\n",
    "                # print(tokenizer.batch_decode(valid_tokens[:, :first_newline]))\n",
    "                new_cache_size = valid_tokens[:, :first_newline].shape[1] + input_ids.shape[1]\n",
    "                valid_tokens = torch.cat((valid_tokens[:, :first_newline], \n",
    "                                          torch.as_tensor([[185, 1202, 185]], dtype=torch.int32, device=input_ids.device), \n",
    "                                          valid_tokens[:, last_newline:]\n",
    "                                         ), dim=-1) \n",
    "                # print(\"Token delta: \", st, valid_tokens.shape[1])\n",
    "                \n",
    "            input_ids = torch.cat((input_ids, valid_tokens), dim=-1)\n",
    "            new_cur_len = input_ids.shape[-1]\n",
    "\n",
    "            if input_ids[0, -1] == 185 and input_ids[0, -2] == 185 and input_ids[0, -3] == 185: # hacky stopping criteria for new-line ending models\n",
    "                break\n",
    "\n",
    "            if print_output:\n",
    "                updated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                # Find and print the newly added text\n",
    "                if updated_text != current_text:\n",
    "                    new_text = updated_text[len(current_text):]\n",
    "                    if len(valid_tokens[0]) > 1:\n",
    "                        color = COLORS[current_color_index]\n",
    "                        print(f\"{color}{new_text}{RESET}\", end='')\n",
    "                        # Update color for next generation\n",
    "                        current_color_index = (current_color_index + 1) % len(COLORS)\n",
    "                    else:\n",
    "                        print(f\"{new_text}\", end='')\n",
    "\n",
    "            new_cache_size = new_cur_len - 1 if new_cache_size == -1 else new_cache_size\n",
    "            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n",
    "\n",
    "        \n",
    "            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "\n",
    "            if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "                break\n",
    "            \n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                break\n",
    "\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                # attentions=decoder_attentions,\n",
    "                # hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a9dcce41-44b2-4260-b844-8348b785d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.greedy_search_pld = greedy_search_pld.__get__(model, type(model))\n",
    "model.greedy_search_pld_diff = greedy_search_pld_diff.__get__(model, type(model))\n",
    "model.greedy_search_pld_custom = greedy_search_pld_custom.__get__(model, type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9527e26-56e7-48ff-bb11-724f88ea0fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```\u001b[31mpython\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Calculate the average\n",
      "average_throughput = np.mean(tokens_per_sec_arr)\n",
      "print(f\"A\u001b[0m\u001b[32mverage Throughput: {average_throughput} tokens/sec\")\n",
      "\n",
      "# Plotting the histogram\n",
      "plt.hist(tokens_per_sec_arr, bins=20, color='blue', edgecolor\u001b[0m\u001b[34m='black', alpha=0.7)\n",
      "plt.title('Histogram of Throughput Values')\n",
      "plt.xlabel('Tokens per Second')\n",
      "plt.ylabel('Frequency')\n",
      "plt.ax\u001b[0m\u001b[35mvline(average_throughput, color='red', linestyle='dashed', linewidth=1)\n",
      "plt.text(average_throughput*0.9, max(plt.ylim())*\u001b[0m\u001b[31m0.9, f'Average: {average_throughput:.2f}', color = 'red')\n",
      "plt.x\u001b[0mlim(0\u001b[32m, max\u001b[0m\u001b[34m(tok\u001b[0m\u001b[35mens_per_sec_arr))\u001b[0m\n",
      "pl\u001b[31mt.show\u001b[0m\u001b[32m()\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "Total time: 0.7391133308410645 seconds\n",
      "Tokens per second: 355.8317635818076 tokens/sec\n",
      "Total tokens generated: 263\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import StoppingCriteriaList, MaxLengthCriteria\n",
    "\n",
    "# Define the variable for max_new_tokens\n",
    "max_new_tokens = 500\n",
    "generate_type = 'pld'\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate the output\n",
    "\n",
    "if generate_type == 'regular':\n",
    "    out = model.generate(inputs=inputs.input_ids, max_new_tokens=max_new_tokens, use_cache=True, pad_token_id=0,\n",
    "                         do_sample=False,\n",
    "                         return_dict_in_generate=True)\n",
    "elif generate_type == 'pld':\n",
    "    out = model.greedy_search_pld(inputs.input_ids, \n",
    "                          attention_mask = inputs.attention_mask,\n",
    "                          stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                          draft_matching_window_size = 3,\n",
    "                          draft_num_candidate_tokens = 50,\n",
    "                          use_cache=True, \n",
    "                          pad_token_id=tokenizer.pad_token_id,\n",
    "                          eos_token_id=tokenizer.eos_token_id,\n",
    "                          return_dict_in_generate=True)\n",
    "elif generate_type == 'custom':\n",
    "    out = model.greedy_search_pld_custom(inputs.input_ids, \n",
    "                                code_inputs.input_ids[0].tolist(),\n",
    "                              attention_mask = inputs.attention_mask,\n",
    "                              stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                              draft_matching_window_size = 3,\n",
    "                              draft_num_candidate_tokens = 50,\n",
    "                              use_cache=True, \n",
    "                              pad_token_id=tokenizer.pad_token_id,\n",
    "                              eos_token_id=tokenizer.eos_token_id,\n",
    "                              return_dict_in_generate=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "out_text = tokenizer.batch_decode(out.sequences, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "num_tokens_generated = len(out.sequences[0]) - len(inputs['input_ids'][0])\n",
    "\n",
    "total_time = end_time - start_time\n",
    "tokens_per_sec = num_tokens_generated / total_time\n",
    "\n",
    "print(f\"\\n\\nTotal time: {total_time} seconds\")\n",
    "print(f\"Tokens per second: {tokens_per_sec} tokens/sec\")\n",
    "print(f\"Total tokens generated: {num_tokens_generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de14076a-0985-42a7-b104-d6f92b5a5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nuprl/CanItEdit\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0997c4ad-c43f-4933-a647-82768495808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'name', 'full_name', 'before', 'after', 'tests', 'instruction_descriptive', 'instruction_lazy', 'taxonomy'],\n",
      "    num_rows: 105\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3527769-5cec-4bac-ad26-4c81f8464738",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_counts = [len(tokens) for tokens in tokenizer(ds['before'])['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61c7f48d-8ab7-4fca-b0a9-f9387845d7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.hist(input_token_counts, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b403d30d-ffb6-4236-938c-82fc0379ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.add_column(\"token_count\", input_token_counts)\n",
    "ds = ds.filter(lambda example: example['token_count'] > 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f297529f-7a97-46d4-bd51-491c09dfc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tokens = [5, 10, 20, 40, 80]\n",
    "max_new_tokens = 4000\n",
    "total_time = {\"diff\": {t: 0 for t in pred_tokens}, \"pld\": {t: 0 for t in pred_tokens}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5fc71c5-817f-4405-ba75-7a9dfa7c2b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                             | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                                                            | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|                                                                                                                                               | 1/31 [00:06<03:22,  6.76s/it]\u001b[A\n",
      "  6%|                                                                                                                                          | 2/31 [00:09<02:10,  4.50s/it]\u001b[A\n",
      " 10%|                                                                                                                                     | 3/31 [01:25<17:13, 36.90s/it]\u001b[A\n",
      " 13%|                                                                                                                                 | 4/31 [01:32<11:25, 25.37s/it]\u001b[A\n",
      " 16%|                                                                                                                            | 5/31 [01:40<08:09, 18.82s/it]\u001b[A\n",
      " 19%|                                                                                                                       | 6/31 [01:46<06:03, 14.54s/it]\u001b[A\n",
      " 23%|                                                                                                                  | 7/31 [01:52<04:42, 11.77s/it]\u001b[A\n",
      " 26%|                                                                                                             | 8/31 [02:48<09:55, 25.88s/it]\u001b[A\n",
      " 29%|                                                                                                         | 9/31 [02:54<07:14, 19.73s/it]\u001b[A\n",
      " 32%|                                                                                                   | 10/31 [03:44<10:09, 29.01s/it]\u001b[A\n",
      " 35%|                                                                                              | 11/31 [03:47<06:59, 20.98s/it]\u001b[A\n",
      " 39%|                                                                                          | 12/31 [03:49<04:50, 15.31s/it]\u001b[A\n",
      " 42%|                                                                                     | 13/31 [05:07<10:18, 34.34s/it]\u001b[A\n",
      " 45%|                                                                                | 14/31 [05:29<08:40, 30.61s/it]\u001b[A\n",
      " 48%|                                                                           | 15/31 [06:22<09:55, 37.22s/it]\u001b[A\n",
      " 52%|                                                                       | 16/31 [06:26<06:50, 27.39s/it]\u001b[A\n",
      " 55%|                                                                  | 17/31 [06:31<04:49, 20.66s/it]\u001b[A\n",
      " 58%|                                                             | 18/31 [06:37<03:28, 16.07s/it]\u001b[A\n",
      " 61%|                                                         | 19/31 [06:46<02:47, 13.93s/it]\u001b[A\n",
      " 65%|                                                    | 20/31 [06:56<02:22, 12.94s/it]\u001b[A\n",
      " 68%|                                               | 21/31 [07:01<01:45, 10.59s/it]\u001b[A\n",
      " 71%|                                          | 22/31 [07:06<01:20,  8.92s/it]\u001b[A\n",
      " 74%|                                      | 23/31 [07:10<00:59,  7.43s/it]\u001b[A\n",
      " 77%|                                 | 24/31 [07:21<00:58,  8.29s/it]\u001b[A\n",
      " 81%|                            | 25/31 [07:25<00:42,  7.12s/it]\u001b[A\n",
      " 84%|                       | 26/31 [07:32<00:34,  6.99s/it]\u001b[A\n",
      " 87%|                   | 27/31 [07:38<00:26,  6.70s/it]\u001b[A\n",
      " 90%|              | 28/31 [07:53<00:27,  9.30s/it]\u001b[A\n",
      " 94%|         | 29/31 [07:56<00:15,  7.51s/it]\u001b[A\n",
      " 97%|    | 30/31 [07:58<00:05,  5.83s/it]\u001b[A\n",
      "100%|| 31/31 [08:07<00:00, 15.71s/it]\u001b[A\n",
      " 20%|                                                                                                                      | 1/5 [08:07<32:28, 487.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'diff': {5: 215.59375340491533, 10: 0, 20: 0, 40: 0, 80: 0}, 'pld': {5: 271.2764290906489, 10: 0, 20: 0, 40: 0, 80: 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                            | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|                                                                                                                                               | 1/31 [00:04<02:29,  4.99s/it]\u001b[A\n",
      "  6%|                                                                                                                                          | 2/31 [00:07<01:44,  3.59s/it]\u001b[A\n",
      " 10%|                                                                                                                                     | 3/31 [01:07<13:46, 29.50s/it]\u001b[A\n",
      " 13%|                                                                                                                                 | 4/31 [01:13<08:59, 19.99s/it]\u001b[A\n",
      " 16%|                                                                                                                            | 5/31 [01:18<06:24, 14.77s/it]\u001b[A\n",
      " 19%|                                                                                                                       | 6/31 [01:24<04:47, 11.52s/it]\u001b[A\n",
      " 23%|                                                                                                                  | 7/31 [01:29<03:47,  9.46s/it]\u001b[A\n",
      " 26%|                                                                                                             | 8/31 [02:05<06:55, 18.06s/it]\u001b[A\n",
      " 29%|                                                                                                         | 9/31 [02:10<05:07, 13.97s/it]\u001b[A\n",
      " 32%|                                                                                                   | 10/31 [02:38<06:23, 18.26s/it]\u001b[A\n",
      " 35%|                                                                                              | 11/31 [02:42<04:39, 13.98s/it]\u001b[A\n",
      " 39%|                                                                                          | 12/31 [02:46<03:26, 10.86s/it]\u001b[A\n",
      " 42%|                                                                                     | 13/31 [03:44<07:32, 25.16s/it]\u001b[A\n",
      " 45%|                                                                                | 14/31 [03:50<05:30, 19.45s/it]\u001b[A\n",
      " 48%|                                                                           | 15/31 [04:24<06:20, 23.81s/it]\u001b[A\n",
      " 52%|                                                                       | 16/31 [04:36<05:03, 20.26s/it]\u001b[A\n",
      " 55%|                                                                  | 17/31 [04:39<03:31, 15.08s/it]\u001b[A\n",
      " 58%|                                                             | 18/31 [04:44<02:36, 12.07s/it]\u001b[A\n",
      " 61%|                                                         | 19/31 [04:51<02:04, 10.40s/it]\u001b[A\n",
      " 65%|                                                    | 20/31 [05:00<01:49,  9.96s/it]\u001b[A\n",
      " 68%|                                               | 21/31 [05:11<01:44, 10.44s/it]\u001b[A\n",
      " 71%|                                          | 22/31 [05:16<01:18,  8.67s/it]\u001b[A\n",
      " 74%|                                      | 23/31 [05:19<00:55,  6.89s/it]\u001b[A\n",
      " 77%|                                 | 24/31 [05:26<00:48,  6.94s/it]\u001b[A\n",
      " 81%|                            | 25/31 [05:29<00:35,  5.86s/it]\u001b[A\n",
      " 84%|                       | 26/31 [05:33<00:26,  5.27s/it]\u001b[A\n",
      " 87%|                   | 27/31 [05:37<00:19,  4.90s/it]\u001b[A\n",
      " 90%|              | 28/31 [05:49<00:21,  7.12s/it]\u001b[A\n",
      " 94%|         | 29/31 [05:53<00:12,  6.01s/it]\u001b[A\n",
      " 97%|    | 30/31 [05:59<00:06,  6.06s/it]\u001b[A\n",
      "100%|| 31/31 [06:05<00:00, 11.81s/it]\u001b[A\n",
      " 40%|                                                                                        | 2/5 [14:13<20:47, 415.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'diff': {5: 215.59375340491533, 10: 168.69893556274474, 20: 0, 40: 0, 80: 0}, 'pld': {5: 271.2764290906489, 10: 197.11340865306556, 20: 0, 40: 0, 80: 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                            | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|                                                                                                                                               | 1/31 [00:04<02:25,  4.84s/it]\u001b[A\n",
      "  6%|                                                                                                                                          | 2/31 [00:07<01:44,  3.62s/it]\u001b[A\n",
      " 10%|                                                                                                                                     | 3/31 [00:59<11:53, 25.47s/it]\u001b[A\n",
      " 13%|                                                                                                                                 | 4/31 [01:01<07:19, 16.29s/it]\u001b[A\n",
      " 16%|                                                                                                                            | 5/31 [01:06<05:15, 12.12s/it]\u001b[A\n",
      " 19%|                                                                                                                       | 6/31 [01:10<04:01,  9.66s/it]\u001b[A\n",
      " 23%|                                                                                                                  | 7/31 [01:16<03:18,  8.27s/it]\u001b[A\n",
      " 26%|                                                                                                             | 8/31 [01:43<05:26, 14.21s/it]\u001b[A\n",
      " 29%|                                                                                                         | 9/31 [01:47<04:03, 11.09s/it]\u001b[A\n",
      " 32%|                                                                                                   | 10/31 [02:07<04:53, 13.98s/it]\u001b[A\n",
      " 35%|                                                                                              | 11/31 [02:10<03:27, 10.40s/it]\u001b[A\n",
      " 39%|                                                                                          | 12/31 [02:12<02:29,  7.85s/it]\u001b[A\n",
      " 42%|                                                                                     | 13/31 [03:09<06:49, 22.77s/it]\u001b[A\n",
      " 45%|                                                                                | 14/31 [03:23<05:44, 20.28s/it]\u001b[A\n",
      " 48%|                                                                           | 15/31 [03:49<05:52, 22.02s/it]\u001b[A\n",
      " 52%|                                                                       | 16/31 [04:00<04:38, 18.57s/it]\u001b[A\n",
      " 55%|                                                                  | 17/31 [04:04<03:18, 14.14s/it]\u001b[A\n",
      " 58%|                                                             | 18/31 [04:08<02:26, 11.25s/it]\u001b[A\n",
      " 61%|                                                         | 19/31 [04:13<01:51,  9.30s/it]\u001b[A\n",
      " 65%|                                                    | 20/31 [04:22<01:41,  9.20s/it]\u001b[A\n",
      " 68%|                                               | 21/31 [04:41<02:02, 12.24s/it]\u001b[A\n",
      " 71%|                                          | 22/31 [04:50<01:40, 11.11s/it]\u001b[A\n",
      " 74%|                                      | 23/31 [04:56<01:17,  9.73s/it]\u001b[A\n",
      " 77%|                                 | 24/31 [05:02<00:58,  8.37s/it]\u001b[A\n",
      " 81%|                            | 25/31 [05:05<00:40,  6.81s/it]\u001b[A\n",
      " 84%|                       | 26/31 [05:09<00:30,  6.04s/it]\u001b[A\n",
      " 87%|                   | 27/31 [05:12<00:21,  5.27s/it]\u001b[A\n",
      " 90%|              | 28/31 [05:24<00:21,  7.11s/it]\u001b[A\n",
      " 94%|         | 29/31 [05:30<00:13,  6.78s/it]\u001b[A\n",
      " 97%|    | 30/31 [05:35<00:06,  6.14s/it]\u001b[A\n",
      "100%|| 31/31 [05:41<00:00, 11.01s/it]\u001b[A\n",
      " 60%|                                                           | 3/5 [19:54<12:43, 381.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'diff': {5: 215.59375340491533, 10: 168.69893556274474, 20: 161.18311476707458, 40: 0, 80: 0}, 'pld': {5: 271.2764290906489, 10: 197.11340865306556, 20: 180.03176750428975, 40: 0, 80: 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                            | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|                                                                                                                                               | 1/31 [00:04<02:13,  4.44s/it]\u001b[A\n",
      "  6%|                                                                                                                                          | 2/31 [00:08<02:10,  4.51s/it]\u001b[A\n",
      " 10%|                                                                                                                                     | 3/31 [01:02<12:38, 27.10s/it]\u001b[A\n",
      " 13%|                                                                                                                                 | 4/31 [01:05<07:51, 17.48s/it]\u001b[A\n",
      " 16%|                                                                                                                            | 5/31 [01:09<05:27, 12.59s/it]\u001b[A\n",
      " 19%|                                                                                                                       | 6/31 [01:14<04:09,  9.97s/it]\u001b[A\n",
      " 23%|                                                                                                                  | 7/31 [01:17<03:08,  7.84s/it]\u001b[A\n",
      " 26%|                                                                                                             | 8/31 [01:39<04:41, 12.24s/it]\u001b[A\n",
      " 29%|                                                                                                         | 9/31 [01:43<03:33,  9.71s/it]\u001b[A\n",
      " 32%|                                                                                                   | 10/31 [01:55<03:35, 10.25s/it]\u001b[A\n",
      " 35%|                                                                                              | 11/31 [01:57<02:36,  7.83s/it]\u001b[A\n",
      " 39%|                                                                                          | 12/31 [01:59<01:56,  6.11s/it]\u001b[A\n",
      " 42%|                                                                                     | 13/31 [02:58<06:35, 21.96s/it]\u001b[A\n",
      " 45%|                                                                                | 14/31 [03:11<05:28, 19.32s/it]\u001b[A\n",
      " 48%|                                                                           | 15/31 [03:30<05:06, 19.18s/it]\u001b[A\n",
      " 52%|                                                                       | 16/31 [03:39<04:04, 16.29s/it]\u001b[A\n",
      " 55%|                                                                  | 17/31 [03:43<02:54, 12.47s/it]\u001b[A\n",
      " 58%|                                                             | 18/31 [03:47<02:10, 10.06s/it]\u001b[A\n",
      " 61%|                                                         | 19/31 [03:51<01:38,  8.20s/it]\u001b[A\n",
      " 65%|                                                    | 20/31 [04:01<01:34,  8.57s/it]\u001b[A\n",
      " 68%|                                               | 21/31 [04:17<01:48, 10.80s/it]\u001b[A\n",
      " 71%|                                          | 22/31 [04:26<01:34, 10.48s/it]\u001b[A\n",
      " 74%|                                      | 23/31 [04:29<01:03,  7.97s/it]\u001b[A\n",
      " 77%|                                 | 24/31 [04:33<00:47,  6.78s/it]\u001b[A\n",
      " 81%|                            | 25/31 [04:36<00:33,  5.66s/it]\u001b[A\n",
      " 84%|                       | 26/31 [04:40<00:25,  5.14s/it]\u001b[A\n",
      " 87%|                   | 27/31 [04:43<00:18,  4.51s/it]\u001b[A\n",
      " 90%|              | 28/31 [04:54<00:19,  6.66s/it]\u001b[A\n",
      " 94%|         | 29/31 [04:58<00:11,  5.71s/it]\u001b[A\n",
      " 97%|    | 30/31 [05:02<00:05,  5.20s/it]\u001b[A\n",
      "100%|| 31/31 [05:06<00:00,  9.89s/it]\u001b[A\n",
      " 80%|                             | 4/5 [25:00<05:52, 352.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'diff': {5: 215.59375340491533, 10: 168.69893556274474, 20: 161.18311476707458, 40: 155.2810067087412, 80: 0}, 'pld': {5: 271.2764290906489, 10: 197.11340865306556, 20: 180.03176750428975, 40: 151.04927372932434, 80: 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                            | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|                                                                                                                                               | 1/31 [00:05<02:36,  5.22s/it]\u001b[A\n",
      "  6%|                                                                                                                                          | 2/31 [00:10<02:27,  5.10s/it]\u001b[A\n",
      " 10%|                                                                                                                                     | 3/31 [01:23<16:58, 36.37s/it]\u001b[A\n",
      " 13%|                                                                                                                                 | 4/31 [01:26<10:21, 23.03s/it]\u001b[A\n",
      " 16%|                                                                                                                            | 5/31 [01:30<06:59, 16.15s/it]\u001b[A\n",
      " 19%|                                                                                                                       | 6/31 [01:36<05:18, 12.72s/it]\u001b[A\n",
      " 23%|                                                                                                                  | 7/31 [01:42<04:16, 10.70s/it]\u001b[A\n",
      " 26%|                                                                                                             | 8/31 [02:06<05:40, 14.81s/it]\u001b[A\n",
      " 29%|                                                                                                         | 9/31 [02:11<04:15, 11.64s/it]\u001b[A\n",
      " 32%|                                                                                                   | 10/31 [02:20<03:45, 10.76s/it]\u001b[A\n",
      " 35%|                                                                                              | 11/31 [02:24<02:54,  8.74s/it]\u001b[A\n",
      " 39%|                                                                                          | 12/31 [02:27<02:17,  7.22s/it]\u001b[A\n",
      " 42%|                                                                                     | 13/31 [03:06<05:03, 16.86s/it]\u001b[A\n",
      " 45%|                                                                                | 14/31 [03:21<04:34, 16.17s/it]\u001b[A\n",
      " 48%|                                                                           | 15/31 [03:37<04:19, 16.19s/it]\u001b[A\n",
      " 52%|                                                                       | 16/31 [03:43<03:15, 13.06s/it]\u001b[A\n",
      " 55%|                                                                  | 17/31 [03:47<02:23, 10.28s/it]\u001b[A\n",
      " 58%|                                                             | 18/31 [03:52<01:51,  8.61s/it]\u001b[A\n",
      " 61%|                                                         | 19/31 [03:55<01:26,  7.18s/it]\u001b[A\n",
      " 65%|                                                    | 20/31 [04:06<01:31,  8.33s/it]\u001b[A\n",
      " 68%|                                               | 21/31 [04:23<01:46, 10.68s/it]\u001b[A\n",
      " 71%|                                          | 22/31 [04:35<01:40, 11.21s/it]\u001b[A\n",
      " 74%|                                      | 23/31 [04:39<01:12,  9.04s/it]\u001b[A\n",
      " 77%|                                 | 24/31 [04:43<00:51,  7.40s/it]\u001b[A\n",
      " 81%|                            | 25/31 [04:46<00:37,  6.22s/it]\u001b[A\n",
      " 84%|                       | 26/31 [04:51<00:28,  5.70s/it]\u001b[A\n",
      " 87%|                   | 27/31 [04:54<00:19,  4.93s/it]\u001b[A\n",
      " 90%|              | 28/31 [05:07<00:22,  7.59s/it]\u001b[A\n",
      " 94%|         | 29/31 [05:15<00:14,  7.47s/it]\u001b[A\n",
      " 97%|    | 30/31 [05:18<00:06,  6.31s/it]\u001b[A\n",
      "100%|| 31/31 [05:27<00:00, 10.56s/it]\u001b[A\n",
      "100%|| 5/5 [30:28<00:00, 365.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'diff': {5: 215.59375340491533, 10: 168.69893556274474, 20: 161.18311476707458, 40: 155.2810067087412, 80: 186.24931568093598}, 'pld': {5: 271.2764290906489, 10: 197.11340865306556, 20: 180.03176750428975, 40: 151.04927372932434, 80: 140.9757472537458}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for pred_token_length in tqdm(pred_tokens):\n",
    "    for row in tqdm(ds):\n",
    "        inputs = tokenizer(f\"## Code Before:\\n{row['before']}\\n## Instruction:\\n{row['instruction_descriptive']}\\n## Code After:\\n\", return_tensors=\"pt\")\n",
    "        code_inputs = tokenizer(row['before'], return_tensors=\"pt\")\n",
    "\n",
    "        # Move all tensor values in the inputs to GPU\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(device)\n",
    "            code_inputs[key] = code_inputs[key].to(device)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        out = model.greedy_search_pld(inputs.input_ids, \n",
    "                          attention_mask = inputs.attention_mask,\n",
    "                          stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                          draft_matching_window_size = 3,\n",
    "                          draft_num_candidate_tokens = pred_token_length,\n",
    "                          use_cache=True, \n",
    "                          pad_token_id=tokenizer.pad_token_id,\n",
    "                          eos_token_id=tokenizer.eos_token_id,\n",
    "                          return_dict_in_generate=True,\n",
    "                        print_output=False)\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        total_time[\"pld\"][pred_token_length] += end_time - start_time\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        out = model.greedy_search_pld_diff(inputs.input_ids, \n",
    "                                    code_inputs.input_ids[0].tolist(),\n",
    "                                  attention_mask = inputs.attention_mask,\n",
    "                                  stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=len(inputs.input_ids[0]) + max_new_tokens)]),\n",
    "                                  draft_matching_window_size = 3,\n",
    "                                  draft_num_candidate_tokens = pred_token_length,\n",
    "                                  use_cache=True, \n",
    "                                  pad_token_id=tokenizer.pad_token_id,\n",
    "                                  eos_token_id=tokenizer.eos_token_id,\n",
    "                                  return_dict_in_generate=True,\n",
    "                                    print_output=False)\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        total_time[\"diff\"][pred_token_length] += end_time - start_time\n",
    "    print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3696f669-ae63-4a95-833d-e65f300a306b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'diff': {5: 215.59375340491533,\n",
       "  10: 168.69893556274474,\n",
       "  20: 161.18311476707458,\n",
       "  40: 155.2810067087412,\n",
       "  80: 186.24931568093598},\n",
       " 'pld': {5: 271.2764290906489,\n",
       "  10: 197.11340865306556,\n",
       "  20: 180.03176750428975,\n",
       "  40: 151.04927372932434,\n",
       "  80: 140.9757472537458}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dd010-e805-49d6-a4a6-566ae7fed811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
